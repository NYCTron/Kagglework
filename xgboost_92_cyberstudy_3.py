# -*- coding: utf-8 -*-
"""Xgboost_92%Cyberstudy-3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16MRUq3r9AS05hd6E5owUYoJY3FbOAkhR

Connect to drive for files
"""

#Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

"""Ignore """

pip install lief==0.11.5

"""ignore """

#pip install git+https://github.com/endgameinc/ember.git

!wget https://pubdata.endgame.com/ember/ember_dataset_2017_2.tar.bz2 --no-check-certificate

# Decompressing a .bz2 file
!bzip2 -d A_ember_dataset_2018_2.tar.bz2

pip install ember

import numpy as np
import pandas as pd
from sklearn import preprocessing
import matplotlib.pyplot as plt
#import ember
import altair as alt

"""CHecking files ignore """

# import module
import fileinput
import time
 
#time at the start of program is noted
start = time.time()
 
#keeps a track of number of lines in the file
count = 0
# for lines in fileinput.input(['/content/drive/MyDrive/Dataset/ember2018/ember_model_2018.txt']):
#     print(lines)
#     count = count + 1

for lines in fileinput.input(['ember_model_2018.txt']):
    print(lines)
    count = count + 1
    
#time at the end of program execution is noted
end = time.time()
 
#total time taken to print the file
print("Execution time in seconds: ",(end - start))
print("No. of lines printed: ",count)

"""Checking files ignore """

import time
 
start = time.time()
count = 0
# with open("/content/drive/MyDrive/Dataset/ember2018/ember_model_2018.txt") as file:
#     for line in file:
#        print(line)
#        count = count + 1

with open("ember_model_2018.txt") as file:
    for line in file:
       print(line)
       count = count + 1

end =  time.time()
print("Execution time in seconds: ",(end-start))
print("No of lines printed: ",count)

"""# Opening file and tranfering to google drive."""

#import json
  
# Opening JSON file
#f1 = open('/content/drive/MyDrive/Dataset/ember2018/train_features_0.jsonl')
  
# returns JSON object as 
# a dictionary
#feature1 = json.load(f1)
  
# Iterating through the json
# list
#or i in feature1['emp_details']:
    #print(i)
  
# Closing file
#feature1.close()

#d = open("/content/drive/MyDrive/Dataset/ember2018/ember_model_2018.txt", "r")
#print(d.read())

"""# New Section

https://www.kaggle.com/code/hafizpradanagemilang/bert-multiclass-classification/data?select=y_train.dat

You can start here
"""

#Read data

# x_train = np.memmap('/content/drive/MyDrive/Dataset/ember2018/X_train.dat', shape=(800000, 2381), dtype=np.float32)
# y_train = np.memmap('/content/drive/MyDrive/Dataset/ember2018/y_train.dat', dtype=np.float32)

x_train = np.memmap('X_train.dat', shape=(800000, 2381), dtype=np.float32)
y_train = np.memmap('y_train.dat', dtype=np.float32)

X_train = np.array(x_train)

#!pip install dataprep

#X_train.head()

print(X_train.shape)

y_train = np.array(y_train)

type(X_train)

type(y_train)

# X_test = np.memmap('/content/drive/MyDrive/Dataset/ember2018/X_test.dat', dtype=np.float32)
# y_test = np.memmap('/content/drive/MyDrive/Dataset/ember2018/y_test.dat', dtype=np.float32)

X_test = np.memmap('X_test.dat', dtype=np.float32)
y_test = np.memmap('y_test.dat', dtype=np.float32)

y_test = np.array(y_test)

X_test = np.array(X_test)

type(y_test)

type(X_test)

X_test.shape

X_test_ = X_test[:-342768760]

X_test_.shape

print(56040 * 2381)

X_test.ndim

"""56040 *  2381 = 133431240

476200000 / 56040 
"""

X_test_ = X_test_.reshape(56040,2381)

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split

# Generate a random dataset for classification
#X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=0, random_state=42)

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a Random Forest classifier with 100 trees
rf = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model to the training data
rf.fit(X_train, y_train)

# reshape X_test 

# Reshape the feature array into a 2D array with one column
#X_2d = X_test.reshape(-1, 1)

# Fit the model to the reshaped feature array and the target array
#rf.fit(X_2d, y)



# Make predictions on the test data

y_pred = rf.predict(X_test_)

# Evaluate the accuracy of the model
#accuracy = rf.score(X_test, y_test)

#print("Accuracy:", accuracy)

y_pred.shape

y_test_ = y_test[:56040]

accuracy = rf.score(X_test_, y_test_)
print("Accuracy:", accuracy)

indices

import matplotlib.pyplot as plt
import numpy as np

# create a list of feature names 
#feature_names = list(x_train.columns) # AttributeError: 'memmap' object has no attribute 'columns'

# get feature importances from the model
importances = rf.feature_importances_

# sort the feature importances in descending order
indices = np.argsort(importances)[::-1]

# calculate the number of features that make up the top 10%
#num_top_features = int(len(feature_names) * 0.1)

# create a bar plot of the top 10% feature importances
plt.figure(figsize=(10, 6))
plt.bar(indices, importances[indices])#[:num_top_features]



import xgboost as xgb
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load the data into a pandas dataframe
#data = pd.read_csv('end_game_malware_data.csv')

# Separate the data into features and labels
#X = data.drop('Label', axis=1)
#y = data['Label']

# Split the data into training and testing sets
#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameters for the XGBoost model
params = {
    'max_depth': 6,
    'learning_rate': 0.1,
    'objective': 'binary:logistic',
    'n_estimators': 100
}

# Train the XGBoost model
model = xgb.XGBClassifier(**params)
model.fit(X_train, y_train)

# Make predictions on the testing set
y_pred2 = model.predict(X_test_)

# Evaluate the accuracy of the model
accuracy2 = accuracy_score(y_test_, y_pred2)

print(f"Accuracy: {accuracy2}")

import matplotlib.pyplot as plt
import numpy as np

# create a list of feature names 
#feature_names = list(x_train.columns) # AttributeError: 'memmap' object has no attribute 'columns'

# get feature importances from the model
importances = model.feature_importances_

# sort the feature importances in descending order
indices = np.argsort(importances)[::-1]

# calculate the number of features that make up the top 10%
#num_top_features = int(len(feature_names) * 0.1)

# create a bar plot of the top 10% feature importances
plt.figure(figsize=(10, 6))
plt.bar(indices, importances[indices])#[:num_top_features]

bucket_name = 'medium_demo_bucket_190710'
!gsutil -m cp -r /content/drive/My\ Drive/Data/* gs://{bucket_name}/



# /content/drive/MyDrive/Dataset/ember_dataset_2018_2.tar.bz2

# importing the "tarfile" module
#import tarfile
  
# open file
#file = tarfile.open('/content/drive/MyDrive/Dataset/ember_dataset_2018_2.tar.bz2')
  
# extracting file
#file.extractall('/content/drive/MyDrive/Dataset')
  
#file.close()

import ember
#ember.create_vectorized_features("/content/drive/MyDrive/Dataset/ember2018")
#ember.create_metadata("/content/drive/MyDrive/Dataset/ember2018")

import ember
data_path = '/content/drive/MyDrive/Dataset/ember2018'
emberdf = ember.read_metadata(data_path)
emberdf.head()

